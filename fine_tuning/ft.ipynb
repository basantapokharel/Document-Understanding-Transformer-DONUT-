{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b552381",
   "metadata": {},
   "source": [
    "STEP 1 — Load Dataset (Invoices Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "008e2144",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 425\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'ground_truth'],\n",
       "        num_rows: 26\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset from Hugging Face Hub\n",
    "dataset = load_dataset(\"katanaml-org/invoices-donut-data-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b498b",
   "metadata": {},
   "source": [
    "Step 2 — Load Pre-trained DONUT Model and Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a66d60",
   "metadata": {},
   "source": [
    "DONUT is a Transformer-based model designed to directly read an image and generate structured text output — like  invoice JSON — without needing separate OCR steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facfe806",
   "metadata": {},
   "source": [
    "It has two main parts :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbbfe75",
   "metadata": {},
   "source": [
    "<b>1. Processor:</b>\n",
    "<ul>\n",
    "<li>Handles input preparation — turns images into pixel tensors the model understands.</li>\n",
    "<li>Handles output preparation — tokenizes JSON strings into token IDs and vice versa.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2910b456",
   "metadata": {},
   "source": [
    "<b>2. Model:</b>\n",
    "<ul>\n",
    "<li>VisionEncoderDecoderModel</li>\n",
    "<li>The decoder generates the JSON tokens</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b765ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "import torch\n",
    "\n",
    "# Load processor (image & tokenizer) and model (vision encoder + text decoder)\n",
    "processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base\",use_fast=True)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base\")\n",
    "\n",
    "# Check if GPU is available, then move model to GPU for faster training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee867aa3",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1f47e",
   "metadata": {},
   "source": [
    "Trainable Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d0302b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 201,852,024\n",
      "Trainable Parameters: 201,852,024\n",
      "Frozen Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Frozen Parameters: {total_params - trainable_params:,}\")\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4746445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 74,180,728\n",
      "Trainable Parameters: 74,180,728\n",
      "Frozen Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8372e150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 127,671,296\n",
      "Trainable Parameters: 127,671,296\n",
      "Frozen Parameters: 0\n"
     ]
    }
   ],
   "source": [
    "count_parameters(model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "379091ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel(\n",
      "  (encoder): DonutSwinModel(\n",
      "    (embeddings): DonutSwinEmbeddings(\n",
      "      (patch_embeddings): DonutSwinPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): DonutSwinEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): DonutSwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.005263158120214939)\n",
      "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): DonutSwinPatchMerging(\n",
      "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): DonutSwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.010526316240429878)\n",
      "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.015789475291967392)\n",
      "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): DonutSwinPatchMerging(\n",
      "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): DonutSwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.021052632480859756)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.02631578966975212)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.031578950583934784)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.03684210777282715)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.04210526496171951)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.04736842215061188)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.05263157933950424)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.057894736528396606)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.06315789371728897)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.06842105090618134)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.0736842155456543)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.07894736528396606)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (12): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.08421052992343903)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (13): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.08947368711233139)\n",
      "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): DonutSwinPatchMerging(\n",
      "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): DonutSwinStage(\n",
      "          (blocks): ModuleList(\n",
      "            (0): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.09473684430122375)\n",
      "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): DonutSwinLayer(\n",
      "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (attention): DonutSwinAttention(\n",
      "                (self): DonutSwinSelfAttention(\n",
      "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "                (output): DonutSwinSelfOutput(\n",
      "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (drop_path): DonutSwinDropPath(p=0.10000000149011612)\n",
      "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (intermediate): DonutSwinIntermediate(\n",
      "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DonutSwinOutput(\n",
      "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      "  )\n",
      "  (decoder): MBartForCausalLM(\n",
      "    (model): MBartDecoderWrapper(\n",
      "      (decoder): MBartDecoder(\n",
      "        (embed_tokens): MBartScaledWordEmbedding(57525, 1024, padding_idx=1)\n",
      "        (embed_positions): MBartLearnedPositionalEmbedding(1538, 1024)\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x MBartDecoderLayer(\n",
      "            (self_attn): MBartAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (activation_fn): GELUActivation()\n",
      "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (encoder_attn): MBartAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (lm_head): Linear(in_features=1024, out_features=57525, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ad3f8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DonutSwinModel(\n",
      "  (embeddings): DonutSwinEmbeddings(\n",
      "    (patch_embeddings): DonutSwinPatchEmbeddings(\n",
      "      (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
      "    )\n",
      "    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): DonutSwinEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): DonutSwinStage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): Identity()\n",
      "            (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.005263158120214939)\n",
      "            (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): DonutSwinPatchMerging(\n",
      "          (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
      "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): DonutSwinStage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.010526316240429878)\n",
      "            (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.015789475291967392)\n",
      "            (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): DonutSwinPatchMerging(\n",
      "          (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
      "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (2): DonutSwinStage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.021052632480859756)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.02631578966975212)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.031578950583934784)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.03684210777282715)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.04210526496171951)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.04736842215061188)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (6): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.05263157933950424)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (7): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.057894736528396606)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (8): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.06315789371728897)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (9): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.06842105090618134)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (10): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.0736842155456543)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (11): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.07894736528396606)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (12): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.08421052992343903)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (13): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.08947368711233139)\n",
      "            (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (downsample): DonutSwinPatchMerging(\n",
      "          (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
      "          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (3): DonutSwinStage(\n",
      "        (blocks): ModuleList(\n",
      "          (0): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.09473684430122375)\n",
      "            (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): DonutSwinLayer(\n",
      "            (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): DonutSwinAttention(\n",
      "              (self): DonutSwinSelfAttention(\n",
      "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "              (output): DonutSwinSelfOutput(\n",
      "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "                (dropout): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (drop_path): DonutSwinDropPath(p=0.10000000149011612)\n",
      "            (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (intermediate): DonutSwinIntermediate(\n",
      "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): DonutSwinOutput(\n",
      "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): AdaptiveAvgPool1d(output_size=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.encoder)  # Vision Encoder (ViT)\n",
    "# print(model.decoder)  # Decoder (Transformer decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a77d814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "embeddings\n",
      "embeddings.patch_embeddings\n",
      "embeddings.patch_embeddings.projection\n",
      "embeddings.norm\n",
      "embeddings.dropout\n",
      "encoder\n",
      "encoder.layers\n",
      "encoder.layers.0\n",
      "encoder.layers.0.blocks\n",
      "encoder.layers.0.blocks.0\n",
      "encoder.layers.0.blocks.0.layernorm_before\n",
      "encoder.layers.0.blocks.0.attention\n",
      "encoder.layers.0.blocks.0.attention.self\n",
      "encoder.layers.0.blocks.0.attention.self.query\n",
      "encoder.layers.0.blocks.0.attention.self.key\n",
      "encoder.layers.0.blocks.0.attention.self.value\n",
      "encoder.layers.0.blocks.0.attention.self.dropout\n",
      "encoder.layers.0.blocks.0.attention.output\n",
      "encoder.layers.0.blocks.0.attention.output.dense\n",
      "encoder.layers.0.blocks.0.attention.output.dropout\n",
      "encoder.layers.0.blocks.0.drop_path\n",
      "encoder.layers.0.blocks.0.layernorm_after\n",
      "encoder.layers.0.blocks.0.intermediate\n",
      "encoder.layers.0.blocks.0.intermediate.dense\n",
      "encoder.layers.0.blocks.0.intermediate.intermediate_act_fn\n",
      "encoder.layers.0.blocks.0.output\n",
      "encoder.layers.0.blocks.0.output.dense\n",
      "encoder.layers.0.blocks.0.output.dropout\n",
      "encoder.layers.0.blocks.1\n",
      "encoder.layers.0.blocks.1.layernorm_before\n",
      "encoder.layers.0.blocks.1.attention\n",
      "encoder.layers.0.blocks.1.attention.self\n",
      "encoder.layers.0.blocks.1.attention.self.query\n",
      "encoder.layers.0.blocks.1.attention.self.key\n",
      "encoder.layers.0.blocks.1.attention.self.value\n",
      "encoder.layers.0.blocks.1.attention.self.dropout\n",
      "encoder.layers.0.blocks.1.attention.output\n",
      "encoder.layers.0.blocks.1.attention.output.dense\n",
      "encoder.layers.0.blocks.1.attention.output.dropout\n",
      "encoder.layers.0.blocks.1.drop_path\n",
      "encoder.layers.0.blocks.1.layernorm_after\n",
      "encoder.layers.0.blocks.1.intermediate\n",
      "encoder.layers.0.blocks.1.intermediate.dense\n",
      "encoder.layers.0.blocks.1.intermediate.intermediate_act_fn\n",
      "encoder.layers.0.blocks.1.output\n",
      "encoder.layers.0.blocks.1.output.dense\n",
      "encoder.layers.0.blocks.1.output.dropout\n",
      "encoder.layers.0.downsample\n",
      "encoder.layers.0.downsample.reduction\n",
      "encoder.layers.0.downsample.norm\n",
      "encoder.layers.1\n",
      "encoder.layers.1.blocks\n",
      "encoder.layers.1.blocks.0\n",
      "encoder.layers.1.blocks.0.layernorm_before\n",
      "encoder.layers.1.blocks.0.attention\n",
      "encoder.layers.1.blocks.0.attention.self\n",
      "encoder.layers.1.blocks.0.attention.self.query\n",
      "encoder.layers.1.blocks.0.attention.self.key\n",
      "encoder.layers.1.blocks.0.attention.self.value\n",
      "encoder.layers.1.blocks.0.attention.self.dropout\n",
      "encoder.layers.1.blocks.0.attention.output\n",
      "encoder.layers.1.blocks.0.attention.output.dense\n",
      "encoder.layers.1.blocks.0.attention.output.dropout\n",
      "encoder.layers.1.blocks.0.drop_path\n",
      "encoder.layers.1.blocks.0.layernorm_after\n",
      "encoder.layers.1.blocks.0.intermediate\n",
      "encoder.layers.1.blocks.0.intermediate.dense\n",
      "encoder.layers.1.blocks.0.intermediate.intermediate_act_fn\n",
      "encoder.layers.1.blocks.0.output\n",
      "encoder.layers.1.blocks.0.output.dense\n",
      "encoder.layers.1.blocks.0.output.dropout\n",
      "encoder.layers.1.blocks.1\n",
      "encoder.layers.1.blocks.1.layernorm_before\n",
      "encoder.layers.1.blocks.1.attention\n",
      "encoder.layers.1.blocks.1.attention.self\n",
      "encoder.layers.1.blocks.1.attention.self.query\n",
      "encoder.layers.1.blocks.1.attention.self.key\n",
      "encoder.layers.1.blocks.1.attention.self.value\n",
      "encoder.layers.1.blocks.1.attention.self.dropout\n",
      "encoder.layers.1.blocks.1.attention.output\n",
      "encoder.layers.1.blocks.1.attention.output.dense\n",
      "encoder.layers.1.blocks.1.attention.output.dropout\n",
      "encoder.layers.1.blocks.1.drop_path\n",
      "encoder.layers.1.blocks.1.layernorm_after\n",
      "encoder.layers.1.blocks.1.intermediate\n",
      "encoder.layers.1.blocks.1.intermediate.dense\n",
      "encoder.layers.1.blocks.1.intermediate.intermediate_act_fn\n",
      "encoder.layers.1.blocks.1.output\n",
      "encoder.layers.1.blocks.1.output.dense\n",
      "encoder.layers.1.blocks.1.output.dropout\n",
      "encoder.layers.1.downsample\n",
      "encoder.layers.1.downsample.reduction\n",
      "encoder.layers.1.downsample.norm\n",
      "encoder.layers.2\n",
      "encoder.layers.2.blocks\n",
      "encoder.layers.2.blocks.0\n",
      "encoder.layers.2.blocks.0.layernorm_before\n",
      "encoder.layers.2.blocks.0.attention\n",
      "encoder.layers.2.blocks.0.attention.self\n",
      "encoder.layers.2.blocks.0.attention.self.query\n",
      "encoder.layers.2.blocks.0.attention.self.key\n",
      "encoder.layers.2.blocks.0.attention.self.value\n",
      "encoder.layers.2.blocks.0.attention.self.dropout\n",
      "encoder.layers.2.blocks.0.attention.output\n",
      "encoder.layers.2.blocks.0.attention.output.dense\n",
      "encoder.layers.2.blocks.0.attention.output.dropout\n",
      "encoder.layers.2.blocks.0.drop_path\n",
      "encoder.layers.2.blocks.0.layernorm_after\n",
      "encoder.layers.2.blocks.0.intermediate\n",
      "encoder.layers.2.blocks.0.intermediate.dense\n",
      "encoder.layers.2.blocks.0.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.0.output\n",
      "encoder.layers.2.blocks.0.output.dense\n",
      "encoder.layers.2.blocks.0.output.dropout\n",
      "encoder.layers.2.blocks.1\n",
      "encoder.layers.2.blocks.1.layernorm_before\n",
      "encoder.layers.2.blocks.1.attention\n",
      "encoder.layers.2.blocks.1.attention.self\n",
      "encoder.layers.2.blocks.1.attention.self.query\n",
      "encoder.layers.2.blocks.1.attention.self.key\n",
      "encoder.layers.2.blocks.1.attention.self.value\n",
      "encoder.layers.2.blocks.1.attention.self.dropout\n",
      "encoder.layers.2.blocks.1.attention.output\n",
      "encoder.layers.2.blocks.1.attention.output.dense\n",
      "encoder.layers.2.blocks.1.attention.output.dropout\n",
      "encoder.layers.2.blocks.1.drop_path\n",
      "encoder.layers.2.blocks.1.layernorm_after\n",
      "encoder.layers.2.blocks.1.intermediate\n",
      "encoder.layers.2.blocks.1.intermediate.dense\n",
      "encoder.layers.2.blocks.1.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.1.output\n",
      "encoder.layers.2.blocks.1.output.dense\n",
      "encoder.layers.2.blocks.1.output.dropout\n",
      "encoder.layers.2.blocks.2\n",
      "encoder.layers.2.blocks.2.layernorm_before\n",
      "encoder.layers.2.blocks.2.attention\n",
      "encoder.layers.2.blocks.2.attention.self\n",
      "encoder.layers.2.blocks.2.attention.self.query\n",
      "encoder.layers.2.blocks.2.attention.self.key\n",
      "encoder.layers.2.blocks.2.attention.self.value\n",
      "encoder.layers.2.blocks.2.attention.self.dropout\n",
      "encoder.layers.2.blocks.2.attention.output\n",
      "encoder.layers.2.blocks.2.attention.output.dense\n",
      "encoder.layers.2.blocks.2.attention.output.dropout\n",
      "encoder.layers.2.blocks.2.drop_path\n",
      "encoder.layers.2.blocks.2.layernorm_after\n",
      "encoder.layers.2.blocks.2.intermediate\n",
      "encoder.layers.2.blocks.2.intermediate.dense\n",
      "encoder.layers.2.blocks.2.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.2.output\n",
      "encoder.layers.2.blocks.2.output.dense\n",
      "encoder.layers.2.blocks.2.output.dropout\n",
      "encoder.layers.2.blocks.3\n",
      "encoder.layers.2.blocks.3.layernorm_before\n",
      "encoder.layers.2.blocks.3.attention\n",
      "encoder.layers.2.blocks.3.attention.self\n",
      "encoder.layers.2.blocks.3.attention.self.query\n",
      "encoder.layers.2.blocks.3.attention.self.key\n",
      "encoder.layers.2.blocks.3.attention.self.value\n",
      "encoder.layers.2.blocks.3.attention.self.dropout\n",
      "encoder.layers.2.blocks.3.attention.output\n",
      "encoder.layers.2.blocks.3.attention.output.dense\n",
      "encoder.layers.2.blocks.3.attention.output.dropout\n",
      "encoder.layers.2.blocks.3.drop_path\n",
      "encoder.layers.2.blocks.3.layernorm_after\n",
      "encoder.layers.2.blocks.3.intermediate\n",
      "encoder.layers.2.blocks.3.intermediate.dense\n",
      "encoder.layers.2.blocks.3.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.3.output\n",
      "encoder.layers.2.blocks.3.output.dense\n",
      "encoder.layers.2.blocks.3.output.dropout\n",
      "encoder.layers.2.blocks.4\n",
      "encoder.layers.2.blocks.4.layernorm_before\n",
      "encoder.layers.2.blocks.4.attention\n",
      "encoder.layers.2.blocks.4.attention.self\n",
      "encoder.layers.2.blocks.4.attention.self.query\n",
      "encoder.layers.2.blocks.4.attention.self.key\n",
      "encoder.layers.2.blocks.4.attention.self.value\n",
      "encoder.layers.2.blocks.4.attention.self.dropout\n",
      "encoder.layers.2.blocks.4.attention.output\n",
      "encoder.layers.2.blocks.4.attention.output.dense\n",
      "encoder.layers.2.blocks.4.attention.output.dropout\n",
      "encoder.layers.2.blocks.4.drop_path\n",
      "encoder.layers.2.blocks.4.layernorm_after\n",
      "encoder.layers.2.blocks.4.intermediate\n",
      "encoder.layers.2.blocks.4.intermediate.dense\n",
      "encoder.layers.2.blocks.4.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.4.output\n",
      "encoder.layers.2.blocks.4.output.dense\n",
      "encoder.layers.2.blocks.4.output.dropout\n",
      "encoder.layers.2.blocks.5\n",
      "encoder.layers.2.blocks.5.layernorm_before\n",
      "encoder.layers.2.blocks.5.attention\n",
      "encoder.layers.2.blocks.5.attention.self\n",
      "encoder.layers.2.blocks.5.attention.self.query\n",
      "encoder.layers.2.blocks.5.attention.self.key\n",
      "encoder.layers.2.blocks.5.attention.self.value\n",
      "encoder.layers.2.blocks.5.attention.self.dropout\n",
      "encoder.layers.2.blocks.5.attention.output\n",
      "encoder.layers.2.blocks.5.attention.output.dense\n",
      "encoder.layers.2.blocks.5.attention.output.dropout\n",
      "encoder.layers.2.blocks.5.drop_path\n",
      "encoder.layers.2.blocks.5.layernorm_after\n",
      "encoder.layers.2.blocks.5.intermediate\n",
      "encoder.layers.2.blocks.5.intermediate.dense\n",
      "encoder.layers.2.blocks.5.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.5.output\n",
      "encoder.layers.2.blocks.5.output.dense\n",
      "encoder.layers.2.blocks.5.output.dropout\n",
      "encoder.layers.2.blocks.6\n",
      "encoder.layers.2.blocks.6.layernorm_before\n",
      "encoder.layers.2.blocks.6.attention\n",
      "encoder.layers.2.blocks.6.attention.self\n",
      "encoder.layers.2.blocks.6.attention.self.query\n",
      "encoder.layers.2.blocks.6.attention.self.key\n",
      "encoder.layers.2.blocks.6.attention.self.value\n",
      "encoder.layers.2.blocks.6.attention.self.dropout\n",
      "encoder.layers.2.blocks.6.attention.output\n",
      "encoder.layers.2.blocks.6.attention.output.dense\n",
      "encoder.layers.2.blocks.6.attention.output.dropout\n",
      "encoder.layers.2.blocks.6.drop_path\n",
      "encoder.layers.2.blocks.6.layernorm_after\n",
      "encoder.layers.2.blocks.6.intermediate\n",
      "encoder.layers.2.blocks.6.intermediate.dense\n",
      "encoder.layers.2.blocks.6.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.6.output\n",
      "encoder.layers.2.blocks.6.output.dense\n",
      "encoder.layers.2.blocks.6.output.dropout\n",
      "encoder.layers.2.blocks.7\n",
      "encoder.layers.2.blocks.7.layernorm_before\n",
      "encoder.layers.2.blocks.7.attention\n",
      "encoder.layers.2.blocks.7.attention.self\n",
      "encoder.layers.2.blocks.7.attention.self.query\n",
      "encoder.layers.2.blocks.7.attention.self.key\n",
      "encoder.layers.2.blocks.7.attention.self.value\n",
      "encoder.layers.2.blocks.7.attention.self.dropout\n",
      "encoder.layers.2.blocks.7.attention.output\n",
      "encoder.layers.2.blocks.7.attention.output.dense\n",
      "encoder.layers.2.blocks.7.attention.output.dropout\n",
      "encoder.layers.2.blocks.7.drop_path\n",
      "encoder.layers.2.blocks.7.layernorm_after\n",
      "encoder.layers.2.blocks.7.intermediate\n",
      "encoder.layers.2.blocks.7.intermediate.dense\n",
      "encoder.layers.2.blocks.7.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.7.output\n",
      "encoder.layers.2.blocks.7.output.dense\n",
      "encoder.layers.2.blocks.7.output.dropout\n",
      "encoder.layers.2.blocks.8\n",
      "encoder.layers.2.blocks.8.layernorm_before\n",
      "encoder.layers.2.blocks.8.attention\n",
      "encoder.layers.2.blocks.8.attention.self\n",
      "encoder.layers.2.blocks.8.attention.self.query\n",
      "encoder.layers.2.blocks.8.attention.self.key\n",
      "encoder.layers.2.blocks.8.attention.self.value\n",
      "encoder.layers.2.blocks.8.attention.self.dropout\n",
      "encoder.layers.2.blocks.8.attention.output\n",
      "encoder.layers.2.blocks.8.attention.output.dense\n",
      "encoder.layers.2.blocks.8.attention.output.dropout\n",
      "encoder.layers.2.blocks.8.drop_path\n",
      "encoder.layers.2.blocks.8.layernorm_after\n",
      "encoder.layers.2.blocks.8.intermediate\n",
      "encoder.layers.2.blocks.8.intermediate.dense\n",
      "encoder.layers.2.blocks.8.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.8.output\n",
      "encoder.layers.2.blocks.8.output.dense\n",
      "encoder.layers.2.blocks.8.output.dropout\n",
      "encoder.layers.2.blocks.9\n",
      "encoder.layers.2.blocks.9.layernorm_before\n",
      "encoder.layers.2.blocks.9.attention\n",
      "encoder.layers.2.blocks.9.attention.self\n",
      "encoder.layers.2.blocks.9.attention.self.query\n",
      "encoder.layers.2.blocks.9.attention.self.key\n",
      "encoder.layers.2.blocks.9.attention.self.value\n",
      "encoder.layers.2.blocks.9.attention.self.dropout\n",
      "encoder.layers.2.blocks.9.attention.output\n",
      "encoder.layers.2.blocks.9.attention.output.dense\n",
      "encoder.layers.2.blocks.9.attention.output.dropout\n",
      "encoder.layers.2.blocks.9.drop_path\n",
      "encoder.layers.2.blocks.9.layernorm_after\n",
      "encoder.layers.2.blocks.9.intermediate\n",
      "encoder.layers.2.blocks.9.intermediate.dense\n",
      "encoder.layers.2.blocks.9.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.9.output\n",
      "encoder.layers.2.blocks.9.output.dense\n",
      "encoder.layers.2.blocks.9.output.dropout\n",
      "encoder.layers.2.blocks.10\n",
      "encoder.layers.2.blocks.10.layernorm_before\n",
      "encoder.layers.2.blocks.10.attention\n",
      "encoder.layers.2.blocks.10.attention.self\n",
      "encoder.layers.2.blocks.10.attention.self.query\n",
      "encoder.layers.2.blocks.10.attention.self.key\n",
      "encoder.layers.2.blocks.10.attention.self.value\n",
      "encoder.layers.2.blocks.10.attention.self.dropout\n",
      "encoder.layers.2.blocks.10.attention.output\n",
      "encoder.layers.2.blocks.10.attention.output.dense\n",
      "encoder.layers.2.blocks.10.attention.output.dropout\n",
      "encoder.layers.2.blocks.10.drop_path\n",
      "encoder.layers.2.blocks.10.layernorm_after\n",
      "encoder.layers.2.blocks.10.intermediate\n",
      "encoder.layers.2.blocks.10.intermediate.dense\n",
      "encoder.layers.2.blocks.10.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.10.output\n",
      "encoder.layers.2.blocks.10.output.dense\n",
      "encoder.layers.2.blocks.10.output.dropout\n",
      "encoder.layers.2.blocks.11\n",
      "encoder.layers.2.blocks.11.layernorm_before\n",
      "encoder.layers.2.blocks.11.attention\n",
      "encoder.layers.2.blocks.11.attention.self\n",
      "encoder.layers.2.blocks.11.attention.self.query\n",
      "encoder.layers.2.blocks.11.attention.self.key\n",
      "encoder.layers.2.blocks.11.attention.self.value\n",
      "encoder.layers.2.blocks.11.attention.self.dropout\n",
      "encoder.layers.2.blocks.11.attention.output\n",
      "encoder.layers.2.blocks.11.attention.output.dense\n",
      "encoder.layers.2.blocks.11.attention.output.dropout\n",
      "encoder.layers.2.blocks.11.drop_path\n",
      "encoder.layers.2.blocks.11.layernorm_after\n",
      "encoder.layers.2.blocks.11.intermediate\n",
      "encoder.layers.2.blocks.11.intermediate.dense\n",
      "encoder.layers.2.blocks.11.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.11.output\n",
      "encoder.layers.2.blocks.11.output.dense\n",
      "encoder.layers.2.blocks.11.output.dropout\n",
      "encoder.layers.2.blocks.12\n",
      "encoder.layers.2.blocks.12.layernorm_before\n",
      "encoder.layers.2.blocks.12.attention\n",
      "encoder.layers.2.blocks.12.attention.self\n",
      "encoder.layers.2.blocks.12.attention.self.query\n",
      "encoder.layers.2.blocks.12.attention.self.key\n",
      "encoder.layers.2.blocks.12.attention.self.value\n",
      "encoder.layers.2.blocks.12.attention.self.dropout\n",
      "encoder.layers.2.blocks.12.attention.output\n",
      "encoder.layers.2.blocks.12.attention.output.dense\n",
      "encoder.layers.2.blocks.12.attention.output.dropout\n",
      "encoder.layers.2.blocks.12.drop_path\n",
      "encoder.layers.2.blocks.12.layernorm_after\n",
      "encoder.layers.2.blocks.12.intermediate\n",
      "encoder.layers.2.blocks.12.intermediate.dense\n",
      "encoder.layers.2.blocks.12.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.12.output\n",
      "encoder.layers.2.blocks.12.output.dense\n",
      "encoder.layers.2.blocks.12.output.dropout\n",
      "encoder.layers.2.blocks.13\n",
      "encoder.layers.2.blocks.13.layernorm_before\n",
      "encoder.layers.2.blocks.13.attention\n",
      "encoder.layers.2.blocks.13.attention.self\n",
      "encoder.layers.2.blocks.13.attention.self.query\n",
      "encoder.layers.2.blocks.13.attention.self.key\n",
      "encoder.layers.2.blocks.13.attention.self.value\n",
      "encoder.layers.2.blocks.13.attention.self.dropout\n",
      "encoder.layers.2.blocks.13.attention.output\n",
      "encoder.layers.2.blocks.13.attention.output.dense\n",
      "encoder.layers.2.blocks.13.attention.output.dropout\n",
      "encoder.layers.2.blocks.13.drop_path\n",
      "encoder.layers.2.blocks.13.layernorm_after\n",
      "encoder.layers.2.blocks.13.intermediate\n",
      "encoder.layers.2.blocks.13.intermediate.dense\n",
      "encoder.layers.2.blocks.13.intermediate.intermediate_act_fn\n",
      "encoder.layers.2.blocks.13.output\n",
      "encoder.layers.2.blocks.13.output.dense\n",
      "encoder.layers.2.blocks.13.output.dropout\n",
      "encoder.layers.2.downsample\n",
      "encoder.layers.2.downsample.reduction\n",
      "encoder.layers.2.downsample.norm\n",
      "encoder.layers.3\n",
      "encoder.layers.3.blocks\n",
      "encoder.layers.3.blocks.0\n",
      "encoder.layers.3.blocks.0.layernorm_before\n",
      "encoder.layers.3.blocks.0.attention\n",
      "encoder.layers.3.blocks.0.attention.self\n",
      "encoder.layers.3.blocks.0.attention.self.query\n",
      "encoder.layers.3.blocks.0.attention.self.key\n",
      "encoder.layers.3.blocks.0.attention.self.value\n",
      "encoder.layers.3.blocks.0.attention.self.dropout\n",
      "encoder.layers.3.blocks.0.attention.output\n",
      "encoder.layers.3.blocks.0.attention.output.dense\n",
      "encoder.layers.3.blocks.0.attention.output.dropout\n",
      "encoder.layers.3.blocks.0.drop_path\n",
      "encoder.layers.3.blocks.0.layernorm_after\n",
      "encoder.layers.3.blocks.0.intermediate\n",
      "encoder.layers.3.blocks.0.intermediate.dense\n",
      "encoder.layers.3.blocks.0.intermediate.intermediate_act_fn\n",
      "encoder.layers.3.blocks.0.output\n",
      "encoder.layers.3.blocks.0.output.dense\n",
      "encoder.layers.3.blocks.0.output.dropout\n",
      "encoder.layers.3.blocks.1\n",
      "encoder.layers.3.blocks.1.layernorm_before\n",
      "encoder.layers.3.blocks.1.attention\n",
      "encoder.layers.3.blocks.1.attention.self\n",
      "encoder.layers.3.blocks.1.attention.self.query\n",
      "encoder.layers.3.blocks.1.attention.self.key\n",
      "encoder.layers.3.blocks.1.attention.self.value\n",
      "encoder.layers.3.blocks.1.attention.self.dropout\n",
      "encoder.layers.3.blocks.1.attention.output\n",
      "encoder.layers.3.blocks.1.attention.output.dense\n",
      "encoder.layers.3.blocks.1.attention.output.dropout\n",
      "encoder.layers.3.blocks.1.drop_path\n",
      "encoder.layers.3.blocks.1.layernorm_after\n",
      "encoder.layers.3.blocks.1.intermediate\n",
      "encoder.layers.3.blocks.1.intermediate.dense\n",
      "encoder.layers.3.blocks.1.intermediate.intermediate_act_fn\n",
      "encoder.layers.3.blocks.1.output\n",
      "encoder.layers.3.blocks.1.output.dense\n",
      "encoder.layers.3.blocks.1.output.dropout\n",
      "pooler\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.encoder.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "750a6455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model\n",
      "model.decoder\n",
      "model.decoder.embed_tokens\n",
      "model.decoder.embed_positions\n",
      "model.decoder.layers\n",
      "model.decoder.layers.0\n",
      "model.decoder.layers.0.self_attn\n",
      "model.decoder.layers.0.self_attn.k_proj\n",
      "model.decoder.layers.0.self_attn.v_proj\n",
      "model.decoder.layers.0.self_attn.q_proj\n",
      "model.decoder.layers.0.self_attn.out_proj\n",
      "model.decoder.layers.0.activation_fn\n",
      "model.decoder.layers.0.self_attn_layer_norm\n",
      "model.decoder.layers.0.encoder_attn\n",
      "model.decoder.layers.0.encoder_attn.k_proj\n",
      "model.decoder.layers.0.encoder_attn.v_proj\n",
      "model.decoder.layers.0.encoder_attn.q_proj\n",
      "model.decoder.layers.0.encoder_attn.out_proj\n",
      "model.decoder.layers.0.encoder_attn_layer_norm\n",
      "model.decoder.layers.0.fc1\n",
      "model.decoder.layers.0.fc2\n",
      "model.decoder.layers.0.final_layer_norm\n",
      "model.decoder.layers.1\n",
      "model.decoder.layers.1.self_attn\n",
      "model.decoder.layers.1.self_attn.k_proj\n",
      "model.decoder.layers.1.self_attn.v_proj\n",
      "model.decoder.layers.1.self_attn.q_proj\n",
      "model.decoder.layers.1.self_attn.out_proj\n",
      "model.decoder.layers.1.activation_fn\n",
      "model.decoder.layers.1.self_attn_layer_norm\n",
      "model.decoder.layers.1.encoder_attn\n",
      "model.decoder.layers.1.encoder_attn.k_proj\n",
      "model.decoder.layers.1.encoder_attn.v_proj\n",
      "model.decoder.layers.1.encoder_attn.q_proj\n",
      "model.decoder.layers.1.encoder_attn.out_proj\n",
      "model.decoder.layers.1.encoder_attn_layer_norm\n",
      "model.decoder.layers.1.fc1\n",
      "model.decoder.layers.1.fc2\n",
      "model.decoder.layers.1.final_layer_norm\n",
      "model.decoder.layers.2\n",
      "model.decoder.layers.2.self_attn\n",
      "model.decoder.layers.2.self_attn.k_proj\n",
      "model.decoder.layers.2.self_attn.v_proj\n",
      "model.decoder.layers.2.self_attn.q_proj\n",
      "model.decoder.layers.2.self_attn.out_proj\n",
      "model.decoder.layers.2.activation_fn\n",
      "model.decoder.layers.2.self_attn_layer_norm\n",
      "model.decoder.layers.2.encoder_attn\n",
      "model.decoder.layers.2.encoder_attn.k_proj\n",
      "model.decoder.layers.2.encoder_attn.v_proj\n",
      "model.decoder.layers.2.encoder_attn.q_proj\n",
      "model.decoder.layers.2.encoder_attn.out_proj\n",
      "model.decoder.layers.2.encoder_attn_layer_norm\n",
      "model.decoder.layers.2.fc1\n",
      "model.decoder.layers.2.fc2\n",
      "model.decoder.layers.2.final_layer_norm\n",
      "model.decoder.layers.3\n",
      "model.decoder.layers.3.self_attn\n",
      "model.decoder.layers.3.self_attn.k_proj\n",
      "model.decoder.layers.3.self_attn.v_proj\n",
      "model.decoder.layers.3.self_attn.q_proj\n",
      "model.decoder.layers.3.self_attn.out_proj\n",
      "model.decoder.layers.3.activation_fn\n",
      "model.decoder.layers.3.self_attn_layer_norm\n",
      "model.decoder.layers.3.encoder_attn\n",
      "model.decoder.layers.3.encoder_attn.k_proj\n",
      "model.decoder.layers.3.encoder_attn.v_proj\n",
      "model.decoder.layers.3.encoder_attn.q_proj\n",
      "model.decoder.layers.3.encoder_attn.out_proj\n",
      "model.decoder.layers.3.encoder_attn_layer_norm\n",
      "model.decoder.layers.3.fc1\n",
      "model.decoder.layers.3.fc2\n",
      "model.decoder.layers.3.final_layer_norm\n",
      "model.decoder.layernorm_embedding\n",
      "model.decoder.layer_norm\n",
      "lm_head\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.decoder.named_modules():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1881adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of encoder layers: 4\n"
     ]
    }
   ],
   "source": [
    "# Access encoder layers\n",
    "encoder_layers = model.encoder.encoder.layers\n",
    "\n",
    "# Count number of layers\n",
    "num_layers = len(encoder_layers)\n",
    "print(f\"Number of encoder layers: {num_layers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0d9d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 has 2 blocks\n",
      "Layer 1 has 2 blocks\n",
      "Layer 2 has 14 blocks\n",
      "Layer 3 has 2 blocks\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(encoder_layers):\n",
    "    print(f\"Layer {i} has {len(layer.blocks)} blocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6013502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of decoder layers: 4\n"
     ]
    }
   ],
   "source": [
    "# Access decoder layers\n",
    "decoder_layers = model.decoder.model.decoder.layers\n",
    "\n",
    "# Count number of layers\n",
    "num_decoder_layers = len(decoder_layers)\n",
    "print(f\"Number of decoder layers: {num_decoder_layers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5a41b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Layer 0 has 8 submodules.\n",
      "Decoder Layer 1 has 8 submodules.\n",
      "Decoder Layer 2 has 8 submodules.\n",
      "Decoder Layer 3 has 8 submodules.\n"
     ]
    }
   ],
   "source": [
    "for idx, layer in enumerate(decoder_layers):\n",
    "    print(f\"Decoder Layer {idx} has {len(list(layer.children()))} submodules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3de066",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8dab67",
   "metadata": {},
   "source": [
    "Step 3 — Preprocess Dataset for DONUT Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4e9c5",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>The model expects pixel tensors (numerical arrays) instead of PIL images.</li>\n",
    "<li>The model expects token IDs (numerical sequences) instead of raw JSON strings.</li>\n",
    "<li>So we must convert both inputs and outputs into numbers for the model to learn.</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b87ad814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    # Convert PIL image to pixel values tensor (for vision encoder)\n",
    "    pixel_values = processor(example[\"image\"], return_tensors=\"pt\").pixel_values[0]\n",
    "\n",
    "    # Tokenize the ground truth JSON string (for text decoder)\n",
    "    labels = processor.tokenizer(\n",
    "        example[\"ground_truth\"],\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94975b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/425 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 425/425 [03:59<00:00,  1.78 examples/s]\n"
     ]
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 8053063680 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:3547\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[0;32m   3546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data \u001b[38;5;129;01mand\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3547\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[39;00m\n\u001b[0;32m   3548\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_writer.py:657\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[1;34m(self, close_stream)\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 657\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_writer.py:510\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    506\u001b[0m         batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    507\u001b[0m             row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[0;32m    508\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[0;32m    509\u001b[0m         ]\n\u001b[1;32m--> 510\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_writer.py:626\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[0m\n\u001b[0;32m    625\u001b[0m typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[1;32m--> 626\u001b[0m arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    627\u001b[0m inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\array.pxi:255\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\array.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_writer.py:243\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    242\u001b[0m     trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to_python_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# use smaller integer precisions if possible\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\array.pxi:375\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\array.pxi:45\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 8053063680 failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m processed_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mground_truth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\dataset_dict.py:944\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, with_split, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc, try_original_type)\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[0;32m    942\u001b[0m     function \u001b[38;5;241m=\u001b[39m bind(function, split)\n\u001b[1;32m--> 944\u001b[0m dataset_dict[split] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtry_original_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtry_original_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_split:\n\u001b[0;32m    966\u001b[0m     function \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunc\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    555\u001b[0m }\n\u001b[0;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_dataset.py:3552\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[0m\n\u001b[0;32m   3550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[0;32m   3551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3552\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tmp_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3554\u001b[0m         tmp_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_writer.py:657\u001b[0m, in \u001b[0;36mArrowWriter.finalize\u001b[1;34m(self, close_stream)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# Re-initializing to empty list for next batch\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 657\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;66;03m# If schema is known, infer features even if no examples were written\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema:\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_writer.py:510\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m         batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    507\u001b[0m             row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[0;32m    508\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[0;32m    509\u001b[0m         ]\n\u001b[1;32m--> 510\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_writer.py:626\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size, try_original_type)\u001b[0m\n\u001b[0;32m    620\u001b[0m         col_try_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    621\u001b[0m             try_features[col]\n\u001b[0;32m    622\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m try_features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m try_features \u001b[38;5;129;01mand\u001b[39;00m try_original_type\n\u001b[0;32m    623\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    624\u001b[0m         )\n\u001b[0;32m    625\u001b[0m         typed_sequence \u001b[38;5;241m=\u001b[39m OptimizedTypedSequence(col_values, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mcol_type, try_type\u001b[38;5;241m=\u001b[39mcol_try_type, col\u001b[38;5;241m=\u001b[39mcol)\n\u001b[1;32m--> 626\u001b[0m         arrays\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyped_sequence\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    627\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    628\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\array.pxi:255\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\array.pxi:117\u001b[0m, in \u001b[0;36mpyarrow.lib._handle_arrow_array_protocol\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\datasets\\arrow_writer.py:243\u001b[0m, in \u001b[0;36mTypedSequence.__arrow_array__\u001b[1;34m(self, type)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    242\u001b[0m     trying_cast_to_python_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to_python_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monly_1d_for_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# use smaller integer precisions if possible\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrying_int_optimization:\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\array.pxi:375\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\array.pxi:45\u001b[0m, in \u001b[0;36mpyarrow.lib._sequence_to_array\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\error.pxi:155\u001b[0m, in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 8053063680 failed"
     ]
    }
   ],
   "source": [
    "# processed_dataset = dataset.map(preprocess, remove_columns=[\"image\", \"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2f0aeb",
   "metadata": {},
   "source": [
    "# GOT error as it tries to load the entire tensor dataset in RAM so rather lets define custom PyTorch Dataset class and preprocess the data by dataloader consisting of smaller chunk of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3f1a6",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>.map() applies the preprocess function on every sample.</li>\n",
    "<li>Removes original columns image and ground_truth because we replace them with numeric tensors.</li>\n",
    "<li>Now the dataset has: pixel_values: the image as a tensor, labels: token IDs of the JSON</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f93798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InvoiceDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, processor):\n",
    "        self.dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        # Process image\n",
    "        pixel_values = self.processor(example[\"image\"], return_tensors=\"pt\").pixel_values.squeeze()\n",
    "        # Process text\n",
    "        labels = self.processor.tokenizer(\n",
    "            example[\"ground_truth\"],\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids.squeeze()\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4283af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = InvoiceDataset(dataset[\"train\"], processor)\n",
    "val_dataset = InvoiceDataset(dataset[\"validation\"], processor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ef7bf",
   "metadata": {},
   "source": [
    "Step 4 — Set Up DataLoader & Training Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff37d6",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>DataLoader loads the data in small batches, processes them, and feeds them to the model.</li>\n",
    "<li>Batching allows GPU to process multiple samples at once, making training much faster.</li>\n",
    "<li>We'll set:<ul>\n",
    "<li>Batch size (number of samples processed per step).</li>\n",
    "<li>Shuffling for training data (helps model generalize better).</li>\n",
    "<li>Collate function to pad sequences in the batch (since JSON lengths vary).</li>\n",
    "</ul>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be18e059",
   "metadata": {},
   "source": [
    "4.a) Create Collate Function (for Padding Labels):<br>\n",
    "<li>Different invoices have different JSON lengths,\n",
    "so we need to pad them to match batch dimensions.\n",
    "</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "110f39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    # Pad labels to the same length\n",
    "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=processor.tokenizer.pad_token_id)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46850d33",
   "metadata": {},
   "source": [
    "4.b) Create Dataloaders:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc52d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# You can adjust batch_size based on your GPU VRAM (start with 1 or 2 if low VRAM)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8259a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2560, 1920])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(batch[\"pixel_values\"].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e01b6",
   "metadata": {},
   "source": [
    " Step 5 — Training Loop for Fine-Tuning DONUT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecd64d7",
   "metadata": {},
   "source": [
    "In training, the model learns to minimize the difference between:\n",
    "<li>Predicted output (JSON)</li>\n",
    "<li>Ground truth (actual JSON)</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942bc74",
   "metadata": {},
   "source": [
    "We use:\n",
    "<li>Loss function → measures this difference (called CrossEntropyLoss here).</li>\n",
    "<li>Optimizer → adjusts the model weights to minimize the loss (here, we use AdamW).</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc5065",
   "metadata": {},
   "source": [
    "Training Steps:\n",
    "<li>Loop through training data (many batches → 1 epoch).</li>\n",
    "<li>For each batch:\n",
    "<li>Move data to GPU.</li>\n",
    "<li>Forward pass: Model predicts output.</li>\n",
    "<li>Calculate loss.</li>\n",
    "<li>Backward pass: Compute gradients.</li>\n",
    "<li>Optimizer updates weights.</li>\n",
    "\n",
    "</li>\n",
    "<li>Optionally evaluate on validation set.</li>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c2677c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.64 GiB is allocated by PyTorch, and 17.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss  \u001b[38;5;66;03m# Model internally computes CrossEntropyLoss\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\vision_encoder_decoder\\modeling_vision_encoder_decoder.py:525\u001b[0m, in \u001b[0;36mVisionEncoderDecoderModel.forward\u001b[1;34m(self, pixel_values, decoder_input_ids, decoder_attention_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 525\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    526\u001b[0m         pixel_values\u001b[38;5;241m=\u001b[39mpixel_values,\n\u001b[0;32m    527\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    528\u001b[0m         output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    529\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    530\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_encoder,\n\u001b[0;32m    531\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    533\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\u001b[38;5;241m*\u001b[39mencoder_outputs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:928\u001b[0m, in \u001b[0;36mDonutSwinModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    922\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdepths))\n\u001b[0;32m    924\u001b[0m embedding_output, input_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    925\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[0;32m    926\u001b[0m )\n\u001b[1;32m--> 928\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    939\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:789\u001b[0m, in \u001b[0;36mDonutSwinEncoder.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m    787\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    794\u001b[0m     hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:718\u001b[0m, in \u001b[0;36mDonutSwinStage.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m    716\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    724\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:647\u001b[0m, in \u001b[0;36mDonutSwinLayer.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[0;32m    642\u001b[0m hidden_states_windows \u001b[38;5;241m=\u001b[39m hidden_states_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, channels)\n\u001b[0;32m    643\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_attn_mask(\n\u001b[0;32m    644\u001b[0m     height_pad, width_pad, dtype\u001b[38;5;241m=\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mhidden_states_windows\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    645\u001b[0m )\n\u001b[1;32m--> 647\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    653\u001b[0m attention_windows \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, channels)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:517\u001b[0m, in \u001b[0;36mDonutSwinAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    512\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    516\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 517\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    519\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:424\u001b[0m, in \u001b[0;36mDonutSwinSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    421\u001b[0m mixed_query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states)\n\u001b[0;32m    423\u001b[0m key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m--> 424\u001b[0m value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    425\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 300.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.64 GiB is allocated by PyTorch, and 17.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "\n",
    "# Set decoder start token ID (this solves your error)\n",
    "model.config.decoder_start_token_id = processor.tokenizer.convert_tokens_to_ids([\"<s>\"])[0]\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3  # You can adjust\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Training mode ON\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        # Move data to GPU/CPU\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss  # Model internally computes CrossEntropyLoss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step (optional)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969c3c3",
   "metadata": {},
   "source": [
    "Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee6e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Ground Truth JSON: {\"gt_parse\": {\"header\": {\"invoice_no\": \"97159829\", \"invoice_date\": \"09/18/2015\", \"seller\": \"Bradley-Andrade 9879 Elizabeth Common Lake Jonathan, RI 12335\", \"client\": \"Castro PLC Unit 9678 Box 9664 DPO AP 69387\", \"seller_tax_id\": \"985-73-8194\", \"client_tax_id\": \"994-72-1270\", \"iban\": \"GB81LZWO32519172531418\"}, \"items\": [{\"item_desc\": \"12\\\" Marble Lapis Inlay Chess Table Top With 2\\\" Pieces & 15\\\" Wooden Stand W537\", \"item_qty\": \"2,00\", \"item_net_price\": \"444,60\", \"item_net_worth\": \"889,20\", \"item_vat\": \"10%\", \"item_gross_worth\": \"978,12\"}], \"summary\": {\"total_net_worth\": \"$ 889,20\", \"total_vat\": \"$ 88,92\", \"total_gross_worth\": \"$ 978,12\"}}}\n"
     ]
    }
   ],
   "source": [
    "# Take a sample from test split\n",
    "sample = dataset[\"test\"][0]  # You can change index for other samples\n",
    "image = sample[\"image\"]\n",
    "ground_truth = sample[\"ground_truth\"]\n",
    "image.show()\n",
    "print(\"Ground Truth JSON:\", ground_truth)  # Reference JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79407c4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 470.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.56 GiB is allocated by PyTorch, and 492.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m processor(image, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mpixel_values\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Generate prediction (greedy decoding)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m predicted_tokens \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(outputs\u001b[38;5;241m.\u001b[39msequences, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted JSON:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, predicted_tokens)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\generation\\utils.py:2430\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2426\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`attention_mask` passed to `generate` must be 2D.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[0;32m   2429\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[39;00m\n\u001b[1;32m-> 2430\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2431\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgeneration_config\u001b[49m\n\u001b[0;32m   2432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2434\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[0;32m   2435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\generation\\utils.py:867\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[0;32m    865\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    866\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[1;32m--> 867\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m encoder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:928\u001b[0m, in \u001b[0;36mDonutSwinModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    922\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdepths))\n\u001b[0;32m    924\u001b[0m embedding_output, input_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    925\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[0;32m    926\u001b[0m )\n\u001b[1;32m--> 928\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    939\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:789\u001b[0m, in \u001b[0;36mDonutSwinEncoder.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m    787\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    794\u001b[0m     hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:718\u001b[0m, in \u001b[0;36mDonutSwinStage.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m    716\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 718\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    722\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    724\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:647\u001b[0m, in \u001b[0;36mDonutSwinLayer.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[0;32m    642\u001b[0m hidden_states_windows \u001b[38;5;241m=\u001b[39m hidden_states_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, channels)\n\u001b[0;32m    643\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_attn_mask(\n\u001b[0;32m    644\u001b[0m     height_pad, width_pad, dtype\u001b[38;5;241m=\u001b[39mhidden_states\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mhidden_states_windows\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    645\u001b[0m )\n\u001b[1;32m--> 647\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    651\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    653\u001b[0m attention_windows \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, channels)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:517\u001b[0m, in \u001b[0;36mDonutSwinAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    512\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    515\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    516\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 517\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    519\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\acer\\Desktop\\DONUT\\env\\lib\\site-packages\\transformers\\models\\donut\\modeling_donut_swin.py:430\u001b[0m, in \u001b[0;36mDonutSwinSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[39;00m\n\u001b[0;32m    428\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query_layer, key_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m--> 430\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_head_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_position_bias_table[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_position_index\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m    433\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m relative_position_bias\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    435\u001b[0m )\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 470.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.56 GiB is allocated by PyTorch, and 492.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Prepare image for model\n",
    "pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "# Generate prediction (greedy decoding)\n",
    "outputs = model.generate(pixel_values, max_length=512, return_dict_in_generate=True)\n",
    "predicted_tokens = processor.batch_decode(outputs.sequences, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Predicted JSON:\\n\", predicted_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afb4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up model prediction\n",
    "predicted_json = processor.token2json(predicted_tokens)\n",
    "print(\"Parsed Prediction:\\n\", predicted_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
